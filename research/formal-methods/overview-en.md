<!-- Title -->
<h2 class="section-title">AiX Lab – Formal Methods Research Themes</h2>
<p class="section-subtitle">
  Building Safe Intelligent Systems based on Verified Environment + Verified Learning
</p>

<h2>1. Core Vision</h2>
<p class="section-text">
  Our overall research vision is to build an <strong>"End-to-End Verified System Pipeline"</strong> that goes beyond simple offline verification—connecting formal verification, automatic code generation, and runtime monitoring for cyber-physical systems (CPS) and intelligent systems.
</p>
<p class="section-text">
  This vision encompasses the following core objectives:
</p>
<ul class="section-text">
  <li>Timed Automata modeling → UPPAAL-based formal verification → TADA normalization → Automatic Go code generation</li>
  <li>Verified models + Auto-generated code + Runtime Safety Monitor → Real-time safety-guaranteed systems</li>
  <li>Safety and reliability enhancement: Model checking, temporal constraint verification, runtime violation detection and blocking (shielding)</li>
  <li>Verified World Models: Verifying that states and transitions generated by World Models do not violate physical or logical constraints</li>
  <li>Safe Reinforcement Learning: Blocking dangerous behaviors through Verified Environment + Formal Safety Shields, providing a safe learning framework</li>
  <li>Counterfactual Simulation under Formal Constraints: Generating only realistic and safe simulations under formal constraints</li>
</ul>

<hr class="section-divider" />

<!-- ============================== -->
<!-- FM-U1                          -->
<!-- ============================== -->
<h3 id="fm-u1">FM-U1. Verified Environment Modeling & Execution</h3>
<p>
  This research axis focuses on precisely formalizing the environment in which intelligent systems interact,
  and ensuring that systems operate only within verified environments.
  It clearly defines temporal and logical constraints of the environment, and guarantees safety up to the execution stage.
</p>

<!-- FM-U1.1 -->
<h4 id="fm-u1-1">FM-U1.1. Timed Automata-based Environment & Mission Modeling</h4>
<p>
  We model temporal, sequential, and constraint conditions of real-world environments using <strong>Timed Automata (TA)</strong>
  to define <strong>safety envelopes</strong> that systems must never violate.
  TA models are automatically verified for safety, liveness, deadlines, etc., using model checkers such as UPPAAL.
</p>

<!-- FM-U1.2 -->
<h4 id="fm-u1-2">FM-U1.2. TADA-based Semantic Normalization + Automatic Code Generation</h4>
<p>
  We normalize the ambiguous temporal semantics of TA using <strong>TADA (Timed Automata with Disjoint Actions)</strong>
  so that model semantics and execution semantics correspond 1:1.
  TADA clearly separates time-transitions and action-transitions,
  and expresses all boundary conditions (invariant violations, x==n, etc.) as states to enhance execution safety.
</p>
<p>
  Normalized TADA models are automatically converted to Go code,
  and a <strong>Runtime Safety Monitor</strong> is generated that
  <strong>detects and blocks safety violations in real-time during execution (shielding)</strong>.
</p>

<!-- FM-U1.3 -->
<h4 id="fm-u1-3">FM-U1.3. Verified Simulation Environment</h4>
<p>
  Based on TA/TADA-verified environments, we construct an execution environment that
  <strong>forces Vision, LLM, RL, and Control modules to always behave only under safety constraints</strong>.
  Dangerous behaviors are immediately blocked,
  structurally preventing AI from forming incorrect policies during learning and inference.
</p>

<hr>

<!-- ============================== -->
<!-- FM-U2                          -->
<!-- ============================== -->
<h3 id="fm-u2">FM-U2. Verified World Models, Embodied AI, and Safe Reinforcement Learning</h3>
<p>
  This research axis integrates model-checking-based safety to ensure that
  Embodied AI, World Models, and Reinforcement Learning do not generate incorrect worlds or learn dangerous policies.
</p>

<!-- FM-U2.1 -->
<h4 id="fm-u2-1">FM-U2.1. Verified World Models</h4>
<p>
  We verify that states and transitions generated by World Models
  do not violate physical or logical constraints.
  This ensures that incorrect dynamics or unrealistic scenarios
  do not contaminate AI's policy learning.
</p>

<!-- FM-U2.2 -->
<h4 id="fm-u2-2">FM-U2.2. Safe RL via Verified Environment + Formal Shields</h4>
<p>
  Dangerous behaviors that occur during RL's exploration and policy-learning processes
  are immediately blocked through <strong>Verified Environment</strong> and
  <strong>Formal Safety Shields</strong>.
  This provides a <strong>Safe Learning Framework</strong> that enables RL
  to learn only within safety constraints.
</p>

<!-- FM-U2.3 -->
<h4 id="fm-u2-3">FM-U2.3. Counterfactual Simulation under Formal Constraints</h4>
<p>
  We impose formal constraints on counterfactual scenarios generated by World Models and Embodied AI
  to ensure that <strong>only realistic and safe simulations are generated</strong>.
  This prevents AI from learning incorrect assumptions or dangerous scenarios.
</p>

<hr>

<!-- Summary -->
<h3>Executive Summary</h3>
<p>
  <strong>FM-U1</strong> is a technology that formally models, verifies, and connects environments to execution,
  ensuring that systems operate only within safe environments regardless of which AI algorithm is used.
</p>
<p>
  <strong>FM-U2</strong> integrates model checking from the learning and simulation stages
  to ensure that Embodied AI, World Models, and RL do not generate dangerous states or policies,
  realizing <strong>Verified Learning</strong>.
</p>
<p>
  The combination of these two technologies forms a
  <strong>Verified Intelligent System</strong> where the entire loop of
  Perception → Reasoning/Planning → Control → Execution → Learning
  is equipped with safety guarantees.
</p>

